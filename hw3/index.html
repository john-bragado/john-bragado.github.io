<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<script>
MathJax = {
  tex: {
    displayMath: [["\\[", "\\]"]],
  },
  svg: {
    displayAlign: "left"
  }
};
</script>

		<script type="text/javascript" id="MathJax-script" async
			src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
		</script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
				font-size: 40px;
			}

			h2 {
				font-size: 30px;
			}

			h2 {
			}

			.container {
				margin: 0 auto;
				padding: 60px 10%;
			}

			code {
				background-color:rgb(255, 250, 250);
				color:rgb(130,10, 30);
				padding-left: 1px;
				padding-right: 1px;
			}

			figure {
				text-align: center;
			}

			figcaption {
				font-style: italic;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}

			.flex-container {
				display: flex;
				gap: 20px; /* spacing between boxes */
				justify-content: center; /* center the items horizontally */
				align-items: center;     /* center the items vertically */
			}

			.box {
				/* width: 200px;
				height: 200px; */
				text-align: left;
				padding: 10px;
				/* border: 2px solid #333;
				background-color: #f0f0f0; */
			}

			.box p {
				text-align: left;
			}

		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: John Bragado, Kevin Hu</div>

		Link to webpage: <a href="https://john-bragado.dev/hw3/">john-bragado.dev/hw3</a>
		<br>
		Link to GitHub repository: <a href="https://github.com/cal-cs184/hw-pathtracer-updated-rays">github.com/cal-cs184/hw-pathtracer-updated-rays</a>
		
		<h2>Overview</h2>
		<p>In this assignment, we implemented Monte Carlo path tracing to simulate realistic lighting in 3D scenes. Our goal was to model how light interacts with surfaces by tracing paths that account for both direct illumination (light coming directly from a source) and also indirect illumination (light reflecting off of other surfaces). Most of our logic centers on sampling and integrating radiance using recursive bounce calculations with the rendering equations. We also implemented both hemisphere and light importance sampling methods. Raytracing is a powerful technique because it mimics the physical behavior of light, allowing us to generate images with natural shadows, soft lighting, and color bleeding effects, and we really enjoyed this homework assignment.</p>

		<h3>John’s thoughts:</h3>
		<p>This homework was very <i>enlightening</i>. Raytracing is something I’ve always been fascinated by and this assignment is the very reason I wanted to take this class in the first place. Getting to combine everything I’ve learned so far (not only in terms of conceptual graphics from this class, but also concepts like recursion from CS 61A and probabilistic algorithms from CS70) into a project with such satisfying visual results was very rewarding to me. Although the homework took a very long time, and debugging was a nightmare at times, it was fun <i>bouncing</i> ideas off of Kevin and seeing our engine render beautifully ray-traced scenes.</p>

		<h3>Kevin’s thoughts:</h3>
		<p>Things that I am most interested in is definitely adaptive sampling as unlike other parts of the assignment that is mostly mechanical and physical based, optimization tasks like this using stats always interests me as these tricks often feel smart and interesting as they are less discussed in general. What I learned the most from this is that it is very important to make sure we understand everything conceptually before getting started, even as a person who learns things faster by playing around with the code, I often find that I jump in too early without all the necessary context to play around everything effectively, many times resulting a code that need major revision after I fully understand everything.</p>

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		<h3>Generating Rays</h3>
		<p>The core rendering loop of our pathtracer starts with the function <code>PathTracer::raytrace_pixel()</code>. The pathtracer must generate and send rays originating from the camera and shooting “through” each pixel on the screen.</p>
		<p>We generate these rays with <code>Camera::generate_rays(x, y)</code>. This function takes in a pair of coordinates in image space and outputs a <code>Ray</code> object in world space. In order to do that, we must generate the ray in camera space and only then can we transform it into world space.</p>
		<p>Inside the <code>Camera::generate_rays(x, y)</code> function, we take the coordinates (x, y) and then apply several transformations to it.<p>
		<ol>
			<li>Subtract <code>x</code> and <code>y</code> by 0.5 in order to “center” the rays relative to the image plane, and then multiply them by 2 in order to normalize the image.</li>
			<li>Multiply <code>x</code> by <code>tan(0.5 * hFov)</code> and <code>y</code> by <code>tan(0.5 * vFov)</code>, respectively.</li>
			<li>Convert the coordinates (which are currently 2-dimensional) into a 3-dimensional vector, with the z-coordinate set to -1. We can imagine the image as a box projected onto the plane z = -1.</li>
		</ol>

		<p>After these transformations, we actually generate a <code>Ray</code> object and initialize it such that its origin exists at (0, 0), and its direction is a normalized vector passing through the point we created, at [x’, y’, -1].</p>
		
		<figure>
			<img src="img2cam.png" width="100%"/>
			<figcaption>Conversion of a coordinate from image space to camera space.</figcaption>
		</figure>

		<p>Now that we have the <code>Ray</code> in camera space, transforming it into world space is simply a matter of matrix multiplication. We are given the 3x3 matrix <code>c2w</code> which, when multiplied by a set of coordinates in camera space, rotates and scales the coordinates to their corresponding coordinates in world space. This brings us to our final transformation from camera space to world space.<p>
		<ol type="1" start="4">
			<li>We create a transformation matrix from c2w by inserting a column with the translation from the world origin to the camera (in world space), and a row with 4 zeroes and a 1. This matrix, when passed as an argument into the <code>transform_by()</code> method of a Ray object, will complete this transformation for us and mutate the Ray so that it exists in world space.</li>
		</ol>

		<figure>
			<img src="cam2world.png" width="100%"/>
			<figcaption>Conversion of a Ray from camera space to world space.</figcaption>
		</figure>

		<p>Now we are able to generate a ray through a pixel coordinate. However, we want to supersample and generate multiple rays for every pixel on the screen in order to minimize noise. We also want some sort of way to get the light information from the points on the objects that our rays intersect with, and update our sample buffer with it.</p>
		<p>Inside of our <code>PathTracer::raytrace_pixel(x, y)</code> function, we generate a number of rays (specified by a parameter <code>ns_aa</code> that the user sets when rendering an image). Each ray is generated using our <code>Camera::generate_rays(x, y)</code> function, where we pass in a coordinate randomly sampled from the 1 by 1 pixel from (x, y) to (x + 1, y +  1). We take all of the rays generated, and average the scene radiance along them to determine the integral of radiance for each pixel. To do this, we:</p>
		<ol>
			<li>Store the coordinates (x, y) as a <code>Vector2D</code> and call it <code>origin</code>, representing the left corner of our pixel.</li>
			<li>Initialize another <code>Vector2D</code> called <code>sample</code>, which is some randomly sampled coordinate (using <code>gridSampler->get_sample()</code>) with minimum values x, y and maximum values x+1, y+1. </li>
			<li>Initialize a <code>Ray</code> with <code>Camera::generate_rays(x, y)</code> using the coordinates in <code>sample</code>.</li>
			<li>Pass in the newly generated ray into <code>Pathtracer::est_radiance_global_illumination(Ray)</code>, obtaining the scene radiance along that ray.</li>
			<li>Repeat steps 2-4 in a for-loop, iterating through it <code>ns_aa</code> times and calculate the integral of radiance for the pixel by averaging each ray’s scene radiance.</li>
		</ol>
		
		<div style="break-after:page"></div>
		<h3>Ray-Primitive Intersections</h3>

		<p>In order to test whether a Ray interests a triangle, we implemented the Möller-Trumbore Algorithm, which is an optimization shown in <a href=”https://cs184.eecs.berkeley.edu/su25/assets/lectures/09-11-ray-tracing+acceleration.pdf”>lecture 9</a>. The algorithm uses properties of barycentric coordinates, a system of coordinates where a coordinate is represented as a linear combination of a triangle’s three vectors at its vertices, in order to determine whether the intersection of the ray with the triangle’s plane is “inside” the triangle. We used the original paper as well as the variables from the lecture 9 slide to guide our implementation.</p>
		<p>The algorithm is derived from two equations. The first is the equation for a ray, \(R(t)=O+tD\), where \(O\) is the origin of the ray, \(D\) is the normalized direction vector, and \(t\) is the (scalar) parameter that specifies a point in ray's direction. The second equation is \(T(b_0, b_1, b_2)=b_0P_0+b_1P_1+b_2P_2\), where the set of three scalars \(b_0, b_1, b_2\) are the barycentric coordinates of a point. Since the vectors \(P_0, P_1, P_2\) are not linearly independent and (as a property of barycentric coordinates) the coordinates all must add up to 1, we can arbitrarily modify one of the scalars to make this a function of two arguments.</p>

		<p>Now we have \(R(t)=O+tD\), and \(T(b_1, b_2) = (1 - b_1 - b_2)P_0 + b_1P_1+ b_2P_2\).</p>
		<p>In order to find the point at which the ray intersects the triangle's plane, and whether the intersection lies <i>within</i> the triangle, we set the two equations equal to each other and get:</p>
		<p>\(O+tD = (1 - b_1 - b_2)P_0 + b_1P_1+ b_2P_2\)</p>

		From here, the Möller-Trumbore paper explains how to manipulate the equation and use Cramer's rule to obtain the following equation from lecture.
		<div class="flex-container">
			<div class="box">
				<p>
				\[
				\begin{bmatrix}
				t \\
				b_1 \\
				b_2
				\end{bmatrix}
				=
				\frac{1}{S_1 \cdot E_1}
				\begin{bmatrix}
				S_2 \cdot E_2 \\
				S_1 \cdot S \\
				S_2 \cdot D
				\end{bmatrix}
				\]
				</p>
			</div>
			<div class="box">
				\[
				\begin{align*}
				\vec{E}_1 &= \vec{P}_1 - \vec{P}_0 \\
				\vec{E}_2 &= \vec{P}_2 - \vec{P}_0 \\
				\end{align*}
				\]
			</div>
			<div class="box">
				\[
				\begin{align*}
				\vec{S}_1 &= \vec{D} \times \vec{E}_2 \\
				\vec{S}_2 &= \vec{S} \times \vec{E}_1
				\end{align*}
				\]
			</div>
			<div class="box">
				\[
				\begin{align*}
				\vec{S}   &= \vec{O} - \vec{P}_0 \\
				\end{align*}
				\]
			</div>
  		</div>

		<p>For our implementation of the algorithm, we decided not to calculate all of the components at the beginning of the function but instead spread checks in between that might return <code>false</code>.</p>

		<ol>
			<li>
				We first calculate the vectors <code>s1</code> and <code>e1</code> as shown in the equations above, and set the result to a variable <code>det</code> (short for determinant). We also check to make sure the determinant is nonzero (since \(\frac{1}{0}\) is undefined) and return <code>false</code> if <code>det == 0</code></li>
			</li>
			<li>Then, we calculate <code>s</code>, and calculate one of our barycentric coordinates <code>b1</code> by multiplying the inverse of <code>det</code> by the dot product of <code>s1</code> and <code>s</code>. Before we proceeding, we return false if <code>b1</code> is less than 0 or greater than 1, since barycentric coordinates that lie within the triangle satisfy the condition that all 3 coordiantes are between 0 and 1. </li>
			<li>Next we do something similar for <code>b2</code>, calculating for it by dividin the dot product of <code>s2</code> and the ray direction <code>D</code> by the inverse of <code>det</code>. Once again, we check to make sure <code>b1 > 0.0 || b1 < 1.0</code> is false before proceeding. </li>
			<li>Now that we are certain that the intersection between the ray and the triangle's plane does in fact lie within the bounds of the triangle, we calculate <code>t</code> by finding the dot product of <code>s2</code> and <code>e2</code>, and dividing that by the inverse of <code>det</code> and return <code>true</code>.</li>
		</ol>

		<p>The same algorithm used to evaluate the intersection in the <code>Triangle::has_intersection(Ray)</code> function used above was also used for <code>Triangle::intersect(Ray, Intersection)</code>, but with the addition of writing to the <code>Intersection</code> object the triangle's properties (the value of <code>t</code>, the <code>Primitive</code> object, the BSDF property, and the face normal). The ray's <code>max_t</code> attribute was also updated so that any intersection with an object further than the current primitive would not show up in front of this one. </p>

		<p>As for evaluating intersections analytically with spheres, we followed a similar process but with a different algorithm. Utilizing <a href="https://gfxcourses.stanford.edu/cs348b/spring22content/media/intersection/rt1_3GyBK6F.pdf">lecture slides</a> from Stanford's Spring 2022 offering of CS348b, we used the following formulas:</p>
		<div class="flex-container">
			<div class="box">
				<p><b>Ray:</b> \(
				r(t) = \mathbf{o} + t\mathbf{d}
				\)</p>				
			</div>
			<div class="box">
				<p><b>Sphere:</b> \(
				\|\mathbf{p} - \mathbf{c}\|^2 - r^2 = 0
				\)</p>
			</div>
		</div>
		<p>To find the intersection between the ray and sphere, we set\[
		p = r(t) = o + td
		\]</p>
		<p>where p is the set of all points on the edge of the sphere. Thus, finding the points where \(p=r(t)\) will allow us to find where the ray and sphere intersect.</p>
		</p>
		<p>\[
		(\mathbf{o} + t\mathbf{d} - \mathbf{c})^2 - r^2 = 0
		\]</p>

		<p>We can now expand the equation into quadratic form and solve for the coefficients \(a\), \(b\), and \(c\) analytically with the quadratic formula.</p>

		<p>\[
		at^2 + bt + c = 0
		\]</p>

		<p>\[
		a = \mathbf{d} \cdot \mathbf{d}
		\]</p>

		<p>\[
		b = 2(\mathbf{o} - \mathbf{c}) \cdot \mathbf{d}
		\]</p>

		<p>\[
		c = ((\mathbf{o} - \mathbf{c}) \cdot (\mathbf{o} - \mathbf{c})) - r^2
		\]</p>

		<p>By solving the quadratic equation, we can find the intersection points between the ray and the sphere. Similarly to with the ray-triangle algorithm, If the discriminant \(b^2 - 4ac\) is negative, then there is no intersection and we return false without needing to do any further calculation. If it is zero, then there is one intersection point (the ray is tangent to the sphere). If it is positive, then there are two intersection points and we assign the smaller to <code>t1</code> and the larger to <code>t2</code>. Finally, we update the <code>Intersection</code> object and the ray's <code>max_t</code> properties accordingly.</p>
		
		<br>
		<p>Below are some renders with debug/normal shading to demonstrate that intersection is implemented properly.</p>
		
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center; padding: 10px;">
				  <img src="CBbunny.png" width="400px"/>
				  <figcaption>CBbunny.png</figcaption>
				</td>
				<td style="text-align: center; padding: 10px;">
				  <img src="CBspheres.png" width="400px"/>
				  <figcaption>CBspheres.png</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<div style="break-after:page"></div>

		<h2>Part 2: Bounding Volume Hierarchy</h2>
		<p>Our task for this part was to implement a function, <code>BVHAccel::construct_bvh()</code> that takes in a set of primitives and a max_leaf size, and recursivelyt constructs a Bounding Volume Hierarchy (BVH) tree. Although the bulk of the logic is implemented in the <code>BVHAccel::construct_bvh()</code> function, we also implemented two helper functions, <code>BVHAccel::calcVariance()</code> and <code>BVHAccel::chooseAxis</code>.</p>
		<p>We took the following steps inside our <code>BVHAccel::construct_bvh()</code> function:</p>
		<ol>
			<li>Given only a set of primitives and a max_leaf_size, we initialize a <code>BBox</code> (bounding box) object.</li>
			<li>We then iterate through every primitive in the set, expanding the bounding box to include all primitives using <code>bbox.expand()</code></li>
			<li>We then construct a new <code>BVHNode</code> object with the <code>BBox</code> we just created and expanded. We also calculate the average coordinates of all primitives in this bounding box and store it in a 3D vector.</li>
		</ol>

		<p>The reason we are storing the mean position of the primitives is for our splitting heuristic. To split each bounding box, we decided to use the variance of the primitives' centroids along each axis. This is because the variance is a measure of how spread out the points are, and we want to <b>split the bounding box along the mean of the axis with the highest variance</b> in order to prevent the bounding boxes from being extremely "flat". By splitting the bounding box along its longest axis, we keep the child nodes as close to a cube as possible.</p> 
		
		<ol start="4">
			<li>If the number of primitives in the current node is at least as large as the <code>max_leaf_size</code>, then the current node is considered an "interior" node and will contain two leaf nodes.</li>
			<ol type="a">
				<li>We created a helper function called <code>calcVariance</code> which takes in a set of primitives and outputs a vector of the variances of the primitives' centroids along each of the three axes. We find which axis has the highest variance and store that to a variable called <code>axis</code></li>
				<li>We then use the <code>std::partition()</code> function to essentially split and reorder the primitives, with all primitives of centroids higher than the mean going to the right and all lower than the mean going to the left.</li>
				<li>We construct two new nodes (recursively) calling <code>BVHAccel::construct_bvh()</code> isnide of itself twice; once for the left node and once for the right node.</li>
			</ol>
			<li>Otherwise, the current node is a "leaf" node and does not have any children, but does have pointers to the primitives passed in</li>
			<li>Finally, we return the current node.</li>
		</ol>

		<p>Now that we aren't checking whether a ray intersects with every single primitive in the scene, we are able to render files with thousands of triangles at much faster speeds.</p>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center; padding: 10px;">
				  <img src="CBdragon.png" width="400px"/>
				  <figcaption>CBdragon.png</figcaption>
				</td>
				<td style="text-align: center; padding: 10px;">
				  <img src="CBlucy.png" width="400px"/>
				  <figcaption>CBlucy.png</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<p>
			Comparing the mesh in cow.dae, we found that even though our BVH implementation increased the time it took to construct the BVH from 0.0010 seconds to 0.0033 seconds, the time it took to actually render slashed dramatically from 20.8426 seconds to 0.0469 seconds. In addition, the average intersections per ray dropped from about 745 per ray to about 1.6, and the average speed in millions of rays per second increased from 20,000 rays per second to 10 million rays per second.
			We see similar results with the other meshes we tested, such as maxplanck.dae and CBlucy.dae, as shown in the table below. Overall, we notice that although the time it takes to build the bounding volume hierarchy structure does tend to add some miliseconds, that added time pales in comparison to the speed and performance gains we make from rendering using BVH. As the data shows, the computer can achieve the same result while checking for hundreds or thousands times less intersections for each ray. This reduction of unecessary computation and memory storage results in thousands of times more rays generated per second, and drastic reductions in rendering time.
			The most extreme example of this is the CBlucy.dae mesh, which has 133,796 primitives. The time it took to render the scene dropped from 16,260.17 seconds to 0.0464 seconds (which is almost the same time as the cow.dae mesh took to render).</p>
		</p>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center; padding: 10px;">
				  <img src="cow.png" width="350px"/>
				  <figcaption>cow.dae</figcaption>
				</td>
				<td style="text-align: center; padding: 10px;">
				  <img src="cowslow.png" width="400px"/>
				  <figcaption>BEFORE implementing BVH</figcaption>
				  <p></p>
				  <img src="cowfast.png" width="400px"/>
				  <figcaption>AFTER implementing BVH</figcaption>
				</td>
			  </tr>
			</table>
		</div>


<table cellpadding="8" cellspacing="0">
  <thead>
    <tr>
      <th>File</th>
	  <th># Primitives</th>
      <th>BVH construction (sec)</th>
      <th>Rendering time (sec)</th>
	  <th>AVG speed (rays / sec)</th>
	  <th>AVG intersections per ray</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>cow.dae</td>
	  <td>5,856</td>
      <td>0.0001 -> 0.0033</td>
      <td>20.8426 -> 0.0469</td>
	  <td>0.023 mil -> 10.12 mil</td>
	  <td>745.53 -> 1.66</td>
    </tr>
    <tr>
      <td>maxplanck.dae</td>
	  <td>50,801</td>
      <td>0.0012 -> 0.0347</td>
      <td>195.5508 -> 0.0567</td>
	  <td>0.025 mil -> 8.25 mil</td>
	  <td>6022.19 -> 2.29</td>
    </tr>
	<tr>
      <td>CBlucy.dae</td>
	  <td>133,796</td>
      <td>0.0044 -> 0.1064</td>
      <td>549.0331 -> 0.0464</td>
	  <td>0.0009 mil -> 10.23 mil</td>
	  <td>16,260.17 -> 0.69</td>
    </tr>
  </tbody>
</table>

		<div style="break-after:page"></div>

		<h2>Part 3: Direct Illumination</h2>
		<p>In <code>estimate_direct_lighting_hemisphere</code>, we calculate the direct lighting at a point by randomly picking directions over the whole hemisphere above the surface. We first set up a coordinate frame based on the surface normal so we can easily work in local space. Then, for each sample, we pick a random direction in the hemisphere, check if that direction hits anything (by tracing a shadow ray), and if not blocked, we evaluate the BSDF to see how much light reflects toward the viewer. We multiply the BSDF value by the cosine of the angle with the normal and divide by the probability of picking that direction (the PDF). After looping through all samples, we average the results. This method works for any scene, but it’s pretty noisy because many of the random directions don’t actually hit lights.</p>
		<p>In <code>estimate_direct_lighting_importance()</code>, we improve performance by only sampling directions that actually point toward lights. For each light in the scene, we sample directions based on how the light emits light — which means we’re more likely to sample useful directions. If the light is a point or directional light (delta light), we just take one sample. For area lights, we take multiple samples (defined by global parameter <code>ns_area_light</code>). For each one, we shoot a shadow ray toward the light, check if it’s blocked, and if not, we compute how much light reflects using the BSDF, cosine term, and the PDF. We average the result over all samples and all lights. Since we’re only sampling directions that contribute to the final color, this method gives us cleaner images with less noise.</p>

		<div style="break-after:page"></div>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="CBbunny_L_64_32.png" width="400px"/>
				  <figcaption>importance sampling</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="spheres_L.png" width="400px"/>
				  <figcaption>importance sampling</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="CBbunny_H_64_32.png" width="400px"/>
				  <figcaption>hemisphere sampling</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="spheres_H.png" width="400px"/>
				  <figcaption>hemisphere sampling</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<p>The noise reduction when switching from hemisphere to importance sampling is especially visible in shadows. As seen below, fewer light rays per area light result in noisier renders overall, but especially in the soft shadows.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="Part3/part3s1l1.png" width="400px"/>
				  <figcaption>1 light ray</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="Part3/part3s1l4.png" width="400px"/>
				  <figcaption>4 light rays</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="Part3/part3s1l16.png" width="400px"/>
				  <figcaption>16 light rays</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="Part3/part3s1l64.png" width="400px"/>
				  <figcaption>64 light rays</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<p>The results from hemisphere sampling are typically much noisier than with importance sampling, which provides us a much "smoother" look overall. Hemisphere sampling tends to have noise throughout the entire scene, but importance sampling has especially noisy soft shadows. This demonstrates that importance sampling appears to produce smoother results in well-lit areas, but noisier results in areas of indirect lighting (the soft shadows) or occluded corners. Hemisphere sampling, on the other hand, appears to be uniformly noisy. In addition, hemisphere sampling gives a "glow" effect around the edge of area lights, which importance sampling does not.</p>

		<div style="break-after:page"></div>

		<h2>Part 4: Global Illumination</h2>

		<p>To implement global illumination in our path tracer, we put most of the logic in the <code>PathTracer::at_least_one_bounce_radiance()</code> function. This function handles both direct and indirect lighting by recursively tracing rays, letting them bounce past just the first intersection. At each bounce, we evaluate the BSDF to sample a new direction and compute the radiance along that new path. We check if the ray intersects a surface, and if it does we call the function again within itself to gather more light recursively. Our first base case we checked for was if <code>r.depth == 0</code>, which would only ever be reached if the user set the <code>max_ray_depth</code>> was set to 0. Otherwise, the primary base case would be returning <code>one_bounce_radiance()</code> for the last bounce. The recursive option starts from the camera and continues bouncing off objects until it reaches its' max depth, where it stops and adds the accumulative luminance.</p>

		<p>We also implemented non-accumulative bouncing. If <code>isAccumBounces</code> is set to false, then instead of calling getting the direct light at every recursive bounce and including it in the total, we only go to the very last bounce and grab the indirect lighting bouncing from one surface, to another, and then to the camera.</p>


		<h3>Global Illumination</h3>

		<table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyAccumBounce5.png" width="300px"/>
					<figcaption>CBbunny.dae</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="Part4/glob.png" width="300px"/>
					<figcaption>CBspheres_lambertian.dae</figcaption>
				</td>
			</tr>
			</tr>
		</table>

		<h3>Direct vs Indirect Lighting</h3>

		<table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyAccumBounce1.png" width="200px"/>
					<figcaption>Only Direct Lighting</figcaption>
				</td>
				<td><large>+</large></td>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyJustIndirect.png" width="200px"/>
					<figcaption>Only Indirect Lighting</figcaption>
				</td>
				<td><large>=</large></td>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyAccumBounce5.png" width="200px"/>
					<figcaption>Direct & Indirect</figcaption>
				</td>
			</tr>
			</tr>
		</table>

		<div style="break-after:page"></div>

		<h3>Accumulative Bouncing</h3>

		<table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyAccumBounce0.png" width="100px"/>
					<!-- <figcaption>1 sample per pixel</figcaption> -->
				</td>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyAccumBounce1.png" width="100px"/>
					<!-- <figcaption>2 sample per pixel</figcaption> -->
				</td>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyAccumBounce2.png" width="100px"/>
					<!-- <figcaption>4 sample per pixel</figcaption> -->
				</td>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyAccumBounce3.png" width="100px"/>
					<!-- <figcaption>1 sample per pixel</figcaption> -->
				</td>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyAccumBounce4.png" width="100px"/>
					<!-- <figcaption>2 sample per pixel</figcaption> -->
				</td>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyAccumBounce5.png" width="100px"/>
					<!-- <figcaption>4 sample per pixel</figcaption> -->
				</td>
			</tr>
			<tr>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyNoAccumBounce0.png" width="100px"/>
					<figcaption>m=0</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyNoAccumBounce1.png" width="100px"/>
					<figcaption>m=1</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyNoAccumBounce2.png" width="100px"/>
					<figcaption>m=2</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyNoAccumBounce3.png" width="100px"/>
					<figcaption>m=3</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyNoAccumBounce4.png" width="100px"/>
					<figcaption>m=4</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyNoAccumBounce5.png" width="100px"/>
					<figcaption>m=5</figcaption>
				</td>
			</tr>
		</table>

		<p>At the second and the third bounce of light, we see the bunny in the scene dimly lit. The "bounce light" that these levels contribute are crucial to the realism that we are trying to achieve with ray tracing. The second and third bounces add some light to the shadows that, in the direct lighting render, would otherwise be completely dark. This brings our render much closer to photorealism.</p>

		<h3>Russian Roulette</h3>

		<table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyRRD0.png" width="200px"/>
					<figcaption>max_ray_depth = 0</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyRRD1.png" width="200px"/>
					<figcaption>max_ray_depth = 1</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyRRD2.png" width="200px"/>
					<figcaption>max_ray_depth = 2</figcaption>
				</td>
			</tr>
			<tr>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyRRD3.png" width="200px"/>
					<figcaption>max_ray_depth = 3</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyRRD4.png" width="200px"/>
					<figcaption>max_ray_depth = 4</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="Part4/CBbunnyRRD100.png" width="200px"/>
					<figcaption>max_ray_depth = 100</figcaption>
				</td>
			</tr>
		</table>
		
		<div style="break-after:page"></div>
		<h3>Samples per pixel</h3>

		<table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
				<td style="text-align: center;">
					<img src="Part4/Part4SamplePerPixelS1.png" width="200px"/>
					<figcaption>1 sample per pixel</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="Part4/Part4SamplePerPixelS2.png" width="200px"/>
					<figcaption>2 samples per pixel</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="Part4/Part4SamplePerPixelS4.png" width="200px"/>
					<figcaption>4 samples per pixel</figcaption>
				</td>
			</tr>
			<tr>
				<td style="text-align: center;">
					<img src="Part4/Part4SamplePerPixelS8.png" width="200px"/>
					<figcaption>8 samples per pixel</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="Part4/Part4SamplePerPixelS16.png" width="200px"/>
					<figcaption>16 samples per pixel</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="Part4/Part4SamplePerPixelS64.png" width="200px"/>
					<figcaption>64 samples per pixel</figcaption>
				</td>
			</tr>
		</table>
			<div style="display: flex; flex-direction: column; align-items: center;">
				<img src="Part4/Part4SamplePerPixelS64.png" width="400px"/>
				<figcaption>1024 sample per pixel</figcaption>
			</div>		

		<div style="break-after:page"></div>

		<h2>Part 5: Adaptive Sampling</h2>
		<p>Adaptive sampling is a technique where we detect the variance of the samples we make and run a Z-test to figure out how confident we are in the current sample. Basically, we check our sample as the algorithm runs. If the results change a lot each time we sample, we take more samples since we're less confident the current ones reflect the actual value. But if all the samples we've got so far give very similar results, we feel confident that even with more samples in the future, the result will stay the same, so we stop sampling.</p>
		<p>In our implementation, we just followed the guidelines. For each sample, I kept track of the s1 and s2 numbers, and computed the standard deviation and mean every time a batch finished. If the Z-test shows the confidence level is higher than our tolerance, we stop and return the result</p>
		
		<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="Part5/blob(Adaptive).png" width="150px"/>
				  <figcaption>blob.dae</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="Part5/CBsphere(Adaptive).png" width="150px"/>
				  <figcaption>CBspheres_lambertian.dae</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="Part5/bunny(Adaptive).png" width="150px"/>
				  <figcaption>CBbunny.dae</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="Part5/dragon(Adaptive).png" width="150px"/>
				  <figcaption>dragon.dae</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="Part5/blob(Adaptive)_rate.png" width="150px"/>
				  <figcaption>blob.dae rate</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="Part5/CBsphere(Adaptive)_rate.png" width="150px"/>
				  <figcaption>CBsphere_lambertian.dae rate</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="Part5/bunny_rate.png" width="150px"/>
				  <figcaption>CBbunny.dae rate</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="Part5/dragon(Adaptive)_rate.png" width="150px"/>
				  <figcaption>dragon.dae rate</figcaption>
				</td>
			  </tr>
			</table>

		<div style="break-after:page"></div>

		<h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		<p>For this homework, we attempted to implement a jittered pixel sampler (challenge level 1).</p>

		<p>Here we implemented Jittered sampling that by adding some a random offset within the pixel, we can see that when using sample size 1 the distribution of the shadow under the bunny with default sampling look to be distributed in a pattern while Jittered Sampling randomizes the noise a little bit so that we cannot perceive this fake pattern in noise created by uniformed sample, also see the wall that in default we have some aliasing of almost notch like cutout but with Jittered sampling, because the randomness while it is not clean it certainly don’t have that patterned noise we are seeing which is more pleasant to human eye as we tend to see patterns more clearly.</p>

		<table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
				<td style="text-align: center;">
				  <img src="extra/BunnyNormalSample.png" width="300px"/>
				  <figcaption>Uniform Sampled Bunny</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="extra/Uniform.png" width="200px"/>
				  <figcaption>Wall corner (uniform)</figcaption>
				</td>
				<td style="text-align: center;">
				   <img src="extra/UniformSampling.png" width="200px"/>
				  <figcaption>Closeup of a soft shadow (uniform)</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="extra/BunnyJitteredSample.png" width="300px"/>
				  <figcaption>Jitter Sampled Bunny</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="extra/Jittered.png" width="200px"/>
				  <figcaption>Wall corner (jittered)</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="extra/JitteredSampling.png" width="200px"/>
				  <figcaption>Closeup of a soft shadow (jittered)</figcaption>
				</td>
			  </tr>
			</table>

		<h3>Acknowledgement of AI</h3>
		<p> Our use of AI for this project was limited to asking LLMs to explain concepts to us on a high level and assisting our debugging process.
			We did not ask AI to write code for us.</p>
	</div>
	</body>
</html>